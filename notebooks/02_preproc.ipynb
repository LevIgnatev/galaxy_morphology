{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f2c8361768cb40",
   "metadata": {},
   "source": [
    "# 02_PREPROCESSING\n",
    "**Date:** 06-10-2025\n",
    "\n",
    "**Goals:** Build a preprocessing pipeline to turn the images into compatible data for the upcoming models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac89d3ca4031a1",
   "metadata": {},
   "source": [
    "Let's build the pipeline function, which will take the manifest filepath in (containing labels and image filepaths), and return the train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd76e0574f99c339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T18:16:34.511297Z",
     "start_time": "2025-11-24T18:16:16.982564Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"labels\"\n",
    "\n",
    "labels_and_paths_csv_fp = DATA_PATH / \"labels_manifest_1000.csv\" #labels filepath\n",
    "\n",
    "labels_fp = PROJECT_ROOT / \"data\" / \"processed\" / \"manifest_train_and_val.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14491aac5afbb06a",
   "metadata": {},
   "source": [
    "First, the function will read and extract the needed columns (filepath, label) from the manifest, and then use the train_test_split() function to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91c44b4c129de37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T08:12:39.580480Z",
     "start_time": "2025-11-08T08:12:38.023858Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(labels_fp) # read the labels csv file\n",
    "labels_and_fp_cols = df[[\"derived_label\", \"filepath\"]] # select needed labels\n",
    "\n",
    "train_df, valid_df = train_test_split(labels_and_fp_cols, # split the data for training (80%) and validation (20%)\n",
    "                                          test_size=0.2,\n",
    "                                          stratify=labels_and_fp_cols[\"derived_label\"],\n",
    "                                          random_state=37)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc93c2b0235f1a",
   "metadata": {},
   "source": [
    "Next, let's translate categorical data (words) to integers. I assign a number to every type of galaxy. For example: 0 corresponds to elliptical, 1 to spiral, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513934765e51bd01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T10:58:28.548962Z",
     "start_time": "2025-10-06T10:58:28.503483Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_to_indexes = {} # create a dictionary with labels and indexes (elliptical=0, spiral=1, etc.)\n",
    "unique_labels = labels_and_fp_cols[\"derived_label\"].nunique()\n",
    "for i in range (unique_labels):\n",
    "    labels_to_indexes[labels_and_fp_cols[\"derived_label\"].unique()[i]] = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205bf128518b235",
   "metadata": {},
   "source": [
    "Now, we need a python list of all the data so far (paths and labels separate lists for each train/validation set). It is a necessary step, as the later functions require this data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484907ef2d9e0d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T10:58:32.334656Z",
     "start_time": "2025-10-06T10:58:32.307616Z"
    }
   },
   "outputs": [],
   "source": [
    "train_paths_list = train_df[\"filepath\"].apply(lambda x: str(PROJECT_ROOT / x)).tolist() # convert dataframes to lists\n",
    "valid_paths_list = valid_df[\"filepath\"].apply(lambda x: str(PROJECT_ROOT / x)).tolist()\n",
    "train_labels_list = []\n",
    "valid_labels_list = []\n",
    "for i in train_df[\"derived_label\"]:\n",
    "    train_labels_list.append(labels_to_indexes[i])\n",
    "for i in valid_df[\"derived_label\"]:\n",
    "    valid_labels_list.append(labels_to_indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffadd6fde415900",
   "metadata": {},
   "source": [
    "It is crucial to define a function that resizes and normalizes every image to sets of numbers between 0 and 1, representing each pixel's intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a059a3347b8242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T10:58:36.619953Z",
     "start_time": "2025-10-06T10:58:36.616116Z"
    }
   },
   "outputs": [],
   "source": [
    "def PREPROCESS(path, label):  # function to flatten the images and one-hot encode the labels\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # cast the image to a tensor with numbers between 0 and 1\n",
    "    labels = tf.one_hot(label, unique_labels)  # one-hot encoding the labels\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0d91be5fbf4d5",
   "metadata": {},
   "source": [
    "Finally, we need to create the datasets, suitable for training, using the previous lists we crafted.\n",
    "\n",
    "AUTOTUNE lets us optimize the process by adding parallel computation.\n",
    "\n",
    "The main data pipeline consists of four steps:\n",
    "- mapping using our earlier written PREPROCESS function\n",
    "- shuffling the data to avoid biased distribution\n",
    "- batch all the data into small chunks\n",
    "- prefetch for optimization (AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27bfb10192a8bf85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T10:58:41.714244Z",
     "start_time": "2025-10-06T10:58:41.206364Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE # AUTOTUNE variable for an optimized performance\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths_list, train_labels_list)) # main data pipeline (map, then the classic shuffle - batch - prefetch) for training data\n",
    "train_ds = (\n",
    "    train_ds.shuffle(buffer_size=len(train_paths_list), seed=37)\n",
    "    .map(PREPROCESS, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_paths_list, valid_labels_list)) # main data pipeline (map, then the shuffle - batch - prefetch) for validation data\n",
    "valid_ds = (\n",
    "    valid_ds.shuffle(buffer_size=len(valid_paths_list), seed=37)\n",
    "    .map(PREPROCESS, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dfb1cff076f808",
   "metadata": {},
   "source": [
    "Now all we have left to do is return train_ds and valid_ds! The preprocessing is complete and the resulting datasets can be passed straight to the model at fitting time.\n",
    "\n",
    "Note: it is only possible to do most of these functions because the dataset has already been checked for quality, and missing filepaths/labels are absent.\n",
    "\n",
    "The full function is saved to data_loader.py.\n",
    "\n",
    "Next, I will build the first baseline classifier model (see 03_baselines.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
